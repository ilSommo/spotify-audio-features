\documentclass[a4paper, 12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{spverbatim}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[capitalise]{cleveref}
\usepackage{minted}
\usepackage{setspace}
\usepackage{subcaption}
\usepackage[style=ieee, backend=biber]{biblatex}
\usepackage{booktabs}
\addbibresource{bib.bib}


\onehalfspacing


\begin{document}


\title{Spotify Audio Features}
\author{Martino Mare Lakota Pulici}
\date{September 6, 2021}
\maketitle
\begin{center}
    \fbox{\includegraphics[width=0.618\textwidth]{images/network.png}}
\end{center}


\clearpage


\tableofcontents


\clearpage


\listoffigures


\clearpage


\section{Introduction}

This project aims to apply probabilistic reasoning to a Bayesian network created using audio features of songs on Spotify. Audio features are attributes that Spotify automatically calculates for each song to describe its mood and character.

In addition to Spotify audio features, the presence of each song in the Billboard Hot 100 Chart \cite{hot-100} is also taken into consideration.

Despite not being the focus of the present work, using the Billboard data in association with the Spotify features might be particularly interesting, since it can give an insight into the characteristics which are more relevant in making a song a commercial success. For an in-depth analysis, a work by Elena Georgieva, Marcella Suta, and Nicholas Burton \cite{hitpredict-presentation, hitpredict-report} is suggested.


\section{Features}

The following features are taken into account for each song:
\begin{itemize}
    \item Loudness: volume in dB;
    \item Energy: perceived energy;
    \item Danceability: regular and loud beat;
    \item Acousticness: presence of acoustic instruments;
    \item Valence: perceived cheerfulness;
    \item Speechiness: amount of spoken words;
    \item Instrumentalness: absence of voice;
    \item Liveness: whether the song is performed live;
    \item Tempo: beats per minute;
    \item Chart: whether the song ended up in the Billboard Hot 100 Chart;
    \item ArtistScore: whether the artist has had a previous hit.
\end{itemize}

All features are expressed with a value ranging from 0 to 1, except for loudness and tempo.

The data of both the Spotify features and the Billboard ones are taken from the \spverb|csv| file included in the work by Elena Georgieva, Marcella Suta, and Nicholas Burton \cite{hitpredict-presentation, hitpredict-report}.


\section{Bayesian network}

In theory, a full joint probability distribution can be used to inspect any probabilistic query of the given data. However, the problem can easily become computationally intractable, given the very high number of possible worlds. To simplify the problem, it is possible to exploit the mutual independence of some variables and create a Bayesian network. A Bayesian network is ``a directed graph in which each node is annotated with quantitative probability information'' \cite{probabilistic-reasoning}. In other words, a direct influence relationship is supposed to exist only between nodes that are connected by an arrow. In this case, the structure of the diagram is created using both correlation analysis and human intuition in evaluating the causal links. The resulting network is shown in \cref{fig:network}.

\begin{figure}
    \centering
    \fbox{\includegraphics[width=\textwidth]{images/network.png}}
    \caption{The Bayesian network}
    \label{fig:network}
\end{figure}


\section{Code}

To start, the \spverb|csv| file is imported as a pandas \spverb|DataFrame|. Then, to deal with the continuity of most of the variables, the data are discretized using the pandas \spverb|cut| function. The number of bins for the discretization is chosen to be 4, but it can be easily changed through the \spverb|BINS| value.

Next, a pgmpy \spverb|BayesianModel| is created specifying all the causal links shown in the network and it is then fit to the dataframe data using the \spverb|BayesianModel.fit| function.


\subsection{Conditional probability distributions}

A conditional probability is defined as the probability of a certain event given some prior knowledge. For example, one could ask the probability distribution of the Danceability variable, given that Speechiness is equal to 1, written as $P\left(\mathrm{Danceability} | \mathrm{Speechiness} = 1\right)$. Using the pgmpy package, CPDs can be calculated with the \spverb|BayesianModel.get_cpds| function. As an example, the Danceability CPD is shown:
\begin{center}
    \begin{tabular}{lrrrr}
        \toprule
        Speechiness                 & 0     & 1     & 2     & 3     \\
        \midrule
        $\mathrm{Danceability} = 0$ & 0.052 & 0.018 & 0.063 & 0.000 \\
        $\mathrm{Danceability} = 1$ & 0.264 & 0.113 & 0.125 & 0.304 \\
        $\mathrm{Danceability} = 2$ & 0.508 & 0.411 & 0.688 & 0.522 \\
        $\mathrm{Danceability} = 3$ & 0.176 & 0.458 & 0.125 & 0.174 \\
        \bottomrule
    \end{tabular}
\end{center}

Unfortunately, the \spverb|BayesianModel.get_cpds| function only returns information of CPDs where the given state is made up of variables that are parents of the query variable. For example, if one wants to know the CPD of $P\left(\mathrm{Valence} | \mathrm{Chart} = 1\right)$, the \spverb|BayesianModel.get_cpds| would not work, since the CPD of the Valence variable uses its parents (i.e. Energy and Danceability) as conditioning variables. To compute CPDs of other variable combinations, inference methods must be used.


\subsection{Variable independence}

To efficiently calculate inferences, it is important to exploit variable independence. In general, ``each variable is conditionally independent of its non-descendants, given its parents'' \cite{probabilistic-reasoning}. For example, Danceability can be considered independent of Energy (a non-descendant), given Speechiness (its parent). This fact can be checked using the \spverb|BayesianModel.is_active_trail| function, which returns \spverb|True| if the variables are dependent and \spverb|False| if they are independent. These two lines respectively print True and False, meaning that Danceability and Energy are not independent if Speechiness is not given:
\begin{minted}[autogobble, breaklines, linenos]{python}
    print(model.is_active_trail('Danceability', 'Energy'))
    print(model.is_active_trail('Danceability', 'Energy', observed='Speechiness'))
\end{minted}

Another important property of Bayesian networks is that ``a node is conditionally independent of all other nodes in the network, given its parents, children, and childrenâ€™s parents'' \cite{probabilistic-reasoning}.

This group of variable is known as Markov blanket and can be computed through the \spverb|BayesianModel.get_markov_blanket| function: as an example, the Danceability Markov blanket returns the list \spverb|['ArtistScore', 'Energy', 'Chart', 'Speechiness', 'Valence', 'Loudness']|, whose correctness can be shown by looking at the network. These two lines respectively print True and False, showing that Danceability and Liveness become independent when the Markov blanket for Danceability is given:
\begin{minted}[autogobble, breaklines, linenos]{python}
    print(model.is_active_trail('Danceability', 'Liveness'))
    print(model.is_active_trail('Danceability', 'Liveness', observed=model.get_markov_blanket('Danceability')))
\end{minted}


\subsection{Inferences}

In this work, three different kinds of inference are tested and compared. The code used for each inference method and the corresponding documentation can be found in the \spverb|functions.py| file.

The first method is exact inference, specifically the variable elimination implementation, which consists in ``do[ing] the calculation once and sav[ing] the results for later use'' \cite{probabilistic-reasoning}.

The second method is the rejection sampling method, an approximate inference algorithm which ``generates samples from the prior distribution specified by the network'' \cite{probabilistic-reasoning} and ``rejects all those that do not match the evidence'' \cite{probabilistic-reasoning}.

The last algorithm is another approximate inference method, likelihood weighted sampling. This algorithm ``avoids the inefficiency of rejection sampling by generating only events that are consistent with the evidence'' \cite{probabilistic-reasoning}.

To start, some simple inferences are computed to check if the model makes sense. Since these conditional probabilities are produced by mere direct conditioning, only exact inference is used.

The first influence to be looked at is that of ArtistScore on Chart:
\begin{center}
    \begin{tabular}{lrr}
        \toprule
        ArtistScore          & 0     & 1     \\
        \midrule
        $\mathrm{Chart} = 0$ & 0.694 & 0.248 \\
        $\mathrm{Chart} = 1$ & 0.306 & 0.752 \\
        \bottomrule
    \end{tabular}
\end{center}
As one may expect, whether an artist has had a previous hit (i.e. if they are famous) brings the probability of a song of theirs to be in the chart from around $30\%$ to more than $75\%$.

Another big influence on the probability of a song to hit the chart is known to be its Loudness:
\begin{center}
    \begin{tabular}{lrrrr}
        \toprule
        Loudness             & 0     & 1     & 2     & 3     \\
        \midrule
        $\mathrm{Chart} = 0$ & 0.811 & 0.876 & 0.692 & 0.551 \\
        $\mathrm{Chart} = 1$ & 0.189 & 0.124 & 0.308 & 0.449 \\
        \bottomrule
    \end{tabular}
\end{center}
Again, the model clearly matches expectations: going from $\mathrm{Loudness} = 0$ to $\mathrm{Loudness} = 3$, the probability of a song to hit the charts goes from less than $20\%$ to almost $50\%$.

The last CPDs used to check the model are that of Energy given Loudness:
\begin{center}
    \begin{tabular}{lrrrr}
        \toprule
        Loudness              & 0     & 1     & 2     & 3     \\
        \midrule
        $\mathrm{Energy} = 0$ & 0.795 & 0.899 & 0.149 & 0.004 \\
        $\mathrm{Energy} = 1$ & 0.068 & 0.075 & 0.436 & 0.099 \\
        $\mathrm{Energy} = 2$ & 0.068 & 0.013 & 0.341 & 0.370 \\
        $\mathrm{Energy} = 3$ & 0.068 & 0.013 & 0.073 & 0.527 \\
        \bottomrule
    \end{tabular}
\end{center}
This time, the influence is even more evident: when Loudness is between 0 and 1, the probability of Energy being 0 goes from 80 to $90\%$. On the contrary, a Loudness of 3 grants more than $50\%$ of the songs to have an Energy of 3, as expected.

To conclude, it is calculated that
\[
    P\left(\mathrm{Chart} = 1 | \mathrm{ArtistScore} = 1, \mathrm{Danceability} = 3, \mathrm{Loudness} = 3\right) \approx 77\%,
\]
which is reasonable, given that a danceable and loud song from a famous artist is exactly what is expected to be found in the hit chart. To contrast it is found that
\[
    P\left(\mathrm{Chart} = 1 | \mathrm{ArtistScore} = 0, \mathrm{Danceability} = 0, \mathrm{Loudness} = 0\right) = 0\%,
\]
again supporting the accuracy of the proposed Bayesian network.

After having checked the likeliness of the model, the influence of Chart on Valence (which are not directly connected) is investigated exploiting the power of the Bayesian network. The CPDs for $P\left(\mathrm{Valence} | \mathrm{Chart} = 1\right)$ computed using the three methods are shown in the following table:
\begin{center}
    \begin{tabular}{lrrr}
        \toprule
        $\mathrm{Chart} = 1$   & Exact & Rejection & Weighted \\
        \midrule
        $\mathrm{Valence} = 0$ & 0.143 & 0.150     & 0.150    \\
        $\mathrm{Valence} = 1$ & 0.274 & 0.267     & 0.268    \\
        $\mathrm{Valence} = 2$ & 0.334 & 0.337     & 0.336    \\
        $\mathrm{Valence} = 3$ & 0.249 & 0.246     & 0.247    \\
        \bottomrule
    \end{tabular}
\end{center}

As a reference, the exact values for $P\left(\mathrm{Valence} | \mathrm{Chart} = 0\right)$ are also computed:
\begin{center}
    \begin{tabular}{lr}
        \toprule
        $\mathrm{Chart} = 0$   & Exact \\
        \midrule
        $\mathrm{Valence} = 0$ & 0.223 \\
        $\mathrm{Valence} = 1$ & 0.295 \\
        $\mathrm{Valence} = 2$ & 0.290 \\
        $\mathrm{Valence} = 3$ & 0.191 \\
        \bottomrule
    \end{tabular}
\end{center}

As one may expect, songs that make the chart tend to be more cheerful, even if there is no direct influence relationship of the Chart variable on Valence. This example shows the power of using a Bayesian network: although no information about the conditional probabilities of $P\left(\mathrm{Valence} | \mathrm{Chart}\right)$ is stored in the initial model, inference methods enable computation of these data, without the need of using full joint probability distributions of intractable sizes.


\subsection{Graphs}

Since rejection sampling and likelihood weighted sampling are numerical computations, their accuracy depends on the number of calculated samples. To compare their performance with the exact values, two different graphs for $P\left(\mathrm{Valence} | \mathrm{Chart} = 1\right)$ are plotted in \cref{fig:probabilities,fig:differences}.

\begin{figure}
    \centering
    \fbox{\includegraphics[width=\textwidth]{images/probabilities.png}}
    \caption{Probabilities comparison}
    \label{fig:probabilities}
\end{figure}

\begin{figure}
    \centering
    \fbox{\includegraphics[width=\textwidth]{images/differences.png}}
    \caption{Differences comparison}
    \label{fig:differences}
\end{figure}

In both cases, the x-axis represents the number of samples on a logarithmic scale, while the y-axes of the two graphs show respectively the absolute probability and the difference from the exact probability value. The number at the top of each subplot represents a different Valence value. It is clear from the plots that as the sampling size increases the approximate estimations converge to the reference exact value, as expected.


\clearpage


\printbibliography[heading=bibintoc]

\end{document}